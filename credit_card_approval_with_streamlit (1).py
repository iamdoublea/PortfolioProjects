# -*- coding: utf-8 -*-
"""Credit_Card_Approval_with_Streamlit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WtjWa1uj14efrCpyuhbsFueA7LrT_5vb

Please find all the Project Details with Full Explanation of Steps

Link - https://docs.google.com/document/d/1eV8kcByJbCWErvdMCMd2yKkZabFSMpJz-i14RlqWsTY/edit?usp=sharing
"""

## importing few neccessary libraries

import numpy as np
import pandas as pd

# for visualisation
import matplotlib.pyplot as plt
import seaborn as sns

## Loading the dataset with the help of Pandas

cc = pd.read_csv('/content/Credit_card.csv')

cc_labels = pd.read_csv('/content/Credit_card_label.csv')

## Looking into few rows of the datasets respectively

cc.head()  #top 5 rows

cc_labels.head()

"""We will initially work and do the neccessary Exploratory Data Analysis with the credit card dataset or 'cc' dataset"""

## Checking the number of rows and columns of the data

cc.shape

# so we got 1548 rows and 18 columns

## checking the datatypes and information of the dataset

cc.info()

# as show we have 2 float dtypes, 8 int64 and object datatypes respectively

#Lets check the Descriptive statistics of the overall dataset

cc.describe()

# We have to say that the column named - Ind_ID , is our primary column, and it will help us to establish the relationship between two tables.

## Checking duplicate values

cc.duplicated().sum()   #to get the sum of duplicated values

# no duolicates are found

## Checking the primary column again for Duplicates

cc['Ind_ID'].duplicated().sum()

# for confirmation, there is no duplicates in the cc table

## We will look for Null values in the dataset

cc.isnull().sum()

# Important - We can columns with few to many Null values. We need to fill and fix those accordingly

## Before making any changes to the original dataset, we will make a copy of the Datasets, so as to preserve the original data.

## Making copy of cc dataset

cc_1 = cc.copy()

## rechecking the new dataset

cc_1.head()

cc_1.shape

## Lets proceed with the Null value fixes, one by one
# getting column names

cc_1.columns

### Ind_ID

cc_1['Ind_ID'].isnull().sum()

#no null value

### GENDER

cc_1['GENDER'].isnull().sum()

# there are 7 null values in 'GENDER'

## Lets see the column in more details

cc_1['GENDER'].value_counts()

## we need to find then Mode value of the column

cc_1['GENDER'].mode()[0]

gender_mode = cc_1['GENDER'].mode()[0]
gender_mode

# the mode value of the Gender column is F = Female. Now, mode tells us the most occuring data point of the respective column.

# Since this is a Categorical column and the number of missing values is 7. So, we will fill the null values with the most occuring value is the column that is female or 'F'.

## Filling the null values with the Mode

cc_1['GENDER'].fillna(gender_mode,inplace=True)

## lets recheck the column to see the null values are there or not

cc_1['GENDER'].isnull().sum()

cc_1['GENDER'].value_counts()

### Car Owner

cc_1['Car_Owner'].value_counts()

### Property Owner

cc_1['Propert_Owner'].value_counts()

### Childern

cc_1['CHILDREN'].value_counts()

cc_1['CHILDREN'].dtype

## Lets check distribution and outliers of Childrem column

sns.boxplot(cc_1['CHILDREN'])  #checking outliers with boxplot
plt.show()

# one extreme outlier is there, that is No.14

# We will have to fix the outliers here.
# We will use the mode value to replace the extreme outlier with it.
# Although it is not an extact Categorical value, but the divsion is made on based of category only, like 1,2 etc, so we will use Mode, ie, to use the most occured number of children

mode_children  = cc_1['CHILDREN'].mode()[0]  # getting the mode
mode_children

# replace the extreme outlier with mode value

cc_1.loc[cc_1['CHILDREN']>10,'CHILDREN']=mode_children

# rechecking the value counts and boxplot for outliers

cc_1['CHILDREN'].value_counts()

sns.boxplot(cc_1['CHILDREN'])
plt.show()

#Major outlier has been fixded, this values can be scaled later on

### Annual Income

cc_1['Annual_income'].value_counts()

## Checking null, as there are quite a few
cc_1['Annual_income'].isnull().sum()

## Lets check the distribution of the AnnualIncome column

sns.histplot(cc_1['Annual_income'],kde=True)  #to create histogram
plt.show()

# A proper Right skewed graph, this mean that majority of values are towards the left side while there are only few extreme values or normal values in the right end.

## lets check the Outliers

sns.boxplot(cc_1['Annual_income'])
plt.show()

# Tons of Extreme values in the Annual income column. Outliers.

## Lets check the descriptive statistics of Annual Income

cc_1['Annual_income'].describe()

# What we can see that there are few low income groups and quite a large number of medium to high income groups
# And we believe that this can be genuine and can happen in real data, as different groups of income earrners can apply for Credit
# So We are deciding to NOT Remove outliers or treat extreme values in Annual Income
# This will maintain the variance and the essence of the real population

# But we will have to fix the Null values.
# Since there are Outliers in the data, we will use Median of Annual Income to fill the Null values

## Before filling null values

cc_1['Annual_income'].isnull().sum()

## Filling null with Median

# Getting the median

median_annual_income = cc_1['Annual_income'].median()
median_annual_income

#Filling null value

cc_1['Annual_income'].fillna(median_annual_income,inplace=True)

## After filling null values

cc_1['Annual_income'].isnull().sum()

# No null values

### Type of Income

## Checking nulls

cc_1['Type_Income'].isnull().sum()

## Checking values of Type Income

cc_1['Type_Income'].value_counts()

### EDUCATION

## Checking null

cc_1['EDUCATION'].isnull().sum()

## Checking values

cc_1['EDUCATION'].value_counts()

### MARITAL STATUS

## Checking null

cc_1['Marital_status'].isnull().sum()

##Checking values

cc_1['Marital_status'].value_counts()

### Housing Type

## Checking null

cc_1['Housing_type'].isnull().sum()

## Checking values

cc_1['Housing_type'].value_counts()

### Birthday Count - given in a backward count from current day = 0, or -1 = yesterday

## Checking null

cc_1['Birthday_count'].isnull().sum()

# 22 null values

## Checking the overall values

cc_1['Birthday_count'].value_counts()

## lets check the distribution of the Birthday_count

sns.histplot(cc_1['Birthday_count'],kde=True)
plt.show()

## lets check the outliers and quantiles

sns.boxplot(cc_1['Birthday_count'])
plt.show()

# no such outliers

## We will have to transform this column to make it useful
# We need to get the Age of the individuals applying for the Credit

# We will need to convert these values into Year

# Converting to Year


# adding a new column named Age

cc_1['Age'] = round(-cc_1['Birthday_count']/365.25)  #The average length of a year in the Julian calendar is 365.25 day, so we will divide it by 365.25

cc_1['Age']

# now lets check the distribution again

sns.histplot(cc_1['Age'],kde=True)
plt.show()

# This has a form non-skewed graph, still we can will use Median, since it will give us a Round number for Age, value of age to fill the blank here
# although both mean,rounded and median value are same

#Before filling null

cc_1['Age'].isnull().sum()

## filling nulls with Mean

# getting median value

median_age = cc_1['Age'].median()
median_age

# filling null

cc_1['Age'].fillna(median_age,inplace=True)

## Changing datatype of Age from float to int64

cc_1['Age'].astype('int64')

#After filling null

cc_1['Age'].isnull().sum()

# no null values

### Employed Days -  Start date of employment. Use backward count from current day (0). Positive value means, individual is currently unemployed.

## checking nulls

cc_1['Employed_days'].isnull().sum()

## Checking values

cc_1['Employed_days'].value_counts()

## checking positve values

cc_1[cc_1['Employed_days']>0]

## Checking the value of '+' employed days

cc_1[cc_1['Employed_days']>0]['Employed_days'].value_counts()

# 261 rows with positive values, that means these candidate are not employed right now.
# So this we can change it to 0 years of experience as an employee, since there is one value for positive, and that is 365243.

## We can change the positive value of 365243 to 0, since it will make no difference

cc_1['Employed_days'].replace(365243,0,inplace=True)

## Checking all of '+' employed days again

cc_1['Employed_days'].value_counts()

## Convert these values into Years of Experience, using the same method

cc_1['Experience'] = round(-cc_1['Employed_days']/365.25)  #The average length of a year in the Julian calendar is 365.25 day, so we will divide it by 365.25

# Checking values
cc_1['Experience'].value_counts()

### Mobile phone

## Checking nulls

cc_1['Mobile_phone'].isnull().sum()

## Checking the values

cc_1['Mobile_phone'].value_counts()

##Checking unique values

cc_1['Mobile_phone'].nunique()

# We have only one unique values for mobile phone column and there is no null values

### Work Phone

## Checking nulls

cc_1['Work_Phone'].isnull().sum()

## Checking values

cc_1['Work_Phone'].value_counts()

# We have got two unique values for this column and got No null values

### Email

##Checking nulls

cc_1['EMAIL_ID'].isnull().sum()

## Checking values

cc_1['EMAIL_ID'].value_counts()

# We have got two unique values for this column and got No null values

### Type Occupations

## Checking Nulls

cc_1['Type_Occupation'].isnull().sum()

# We have 488 Null values in this column

## Lets check the values

cc_1['Type_Occupation'].value_counts(dropna=False)

# We need to fill the Null values here , but this is a column of Occupation and we cannot just guess and fill the data, also we don't have access to additional informations, so filling with Mode value is just not psosible here.
# So, we will have to go with the option of Filling the Null rows with 'Unknown' or 'Not Specified'.

## Filling the null values with 'Not Specified' values

cc_1['Type_Occupation'].fillna('Not Specified',inplace=True)

## Rechecking for null values

cc_1['Type_Occupation'].isnull().sum()

## Reching values

cc_1['Type_Occupation'].value_counts(dropna=False)

## Checking the basic distribution of the values

sns.countplot(cc_1["Type_Occupation"], legend=False)  #using a countplot to plot the counts
plt.xlabel('Count')
plt.ylabel('Occupation Type')
plt.title('Occupation Type with their Counts')
plt.show()

# From the above chart we can see that the overall ratio of the applicants for 'IT staff' and 'Realty agents' are pretty low as compared to others.

### Family Members

## Checking Nulls

cc_1['Family_Members'].isnull().sum()

## Checking values

cc_1['Family_Members'].value_counts()

cc_1.head()

"""### Now we will check the data of the Label dataset individually"""

### Expling the Credit Card Labels dataset

cc_labels.head()  #checking first few columns

## Checkig last few columns
cc_labels.tail()

## Checking the shape

cc_labels.shape

# Both the data set has the same number of rows, so it would be good enough to merge them toegther

### Ind_ID

## Checking for nulls

cc_labels['Ind_ID'].isnull().sum()

# no null values

## Checking for duplicated values

cc_labels['Ind_ID'].duplicated().sum()

# No duplicate values

### Target Variable - label

## Label: 0 is application approved and 1 is application rejected.

## Checking for Null

cc_labels['label'].isnull().sum()

# The label column has no null values and also it is in the right format to be used in the model

## Checking the balance of the target variable

cc_labels['label'].value_counts()

# Note: Not a balanced data, so recall, precision and f1 score would be a better way to finally measure teh outcome of the models.
# But as the goal, of the final project we need to the find the model with the best of accuracy, thus we will make the dataset balanced later, to get the reults.

"""Individually both the Datasets have been Explored and Some data issues have been fixed."""

#### MERGING THE 2 Dataset for Better Exploration

df = pd.merge(cc_1,cc_labels, how='inner',on=['Ind_ID'])     # we are using inner join to get the right result and also will be joining on Id column

### Checking the new merged dataset

df.head()

df.shape

# The shape has been maintained

"""### Lets explore all these Graphically

Q4) Identify important patterns in your data using the EDA approach to justify your findings.

Answers abouts relationship and findings have been given under each plots.
"""

### Checking Occupation with Approval ratios

sns.barplot(df,x='Type_Occupation',y='label',ci=None)
plt.title('The ratio of approvals for type of occupations')
plt.xlabel('Occupation')
plt.ylabel('Status')
plt.xticks(rotation=90)
plt.show()

# We can see from the above graph, that the approval rate of the 'IT staff' individuals are quite high as compared to others.

## Checking the number of applicants per occupation group
sns.countplot(df['Type_Occupation'])
plt.title('The number of applicants')
plt.xlabel('Occupation')
plt.xticks(rotation=90)
plt.show()

## Insights
# The approval rate for IT staff is high, although the application by IT staff is also less when compared.
# For HR staff,Secretaries and Realty agents, the approval seems to be less, although there is almost no application from Realty agents
# The approval rate of laborers seems to be less.

### Creating a Age group column for more insight

## Defining a function

def age_group(n):
  if n <= 20:
    return '18-20'
  elif n>20 and n<=30:
    return '21-30'
  elif n>30 and n<=40:
    return '31-40'
  elif n>40 and n<=50:
    return '41-50'
  elif n>50 and n<=61:
    return '51-60'
  else:
    return '60+'

##Applying the Age group funtion in the age column to create a new column
df['Age_Group'] = df['Age'].apply(age_group)

df.head()

## Checking the number of applicants per age group

sns.countplot(df['Age_Group'])
plt.show()

## Checking the number of successful applicant per age group

sns.countplot(df,x='Age_Group',hue='label',palette='twilight')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.title('Approval Numbers per Age Group')
plt.show()

## We can see in the above two graphs that the number of approved applications are most high in 31-40 range, followed by 41-50, and although the rate of Not approved of all of these age group is almost same.
# So, Age is also somewhat important in the decision.

## Checking the number of successful applicant per Income Type

sns.countplot(df,x='Type_Income',hue='label')
plt.xlabel('Income Type')
plt.ylabel('Count')
plt.title('Approval Numbers per Income Type')
plt.xticks(rotation=45)
plt.show()

## We can see that the approved ratio and the number of applicant for the Working class is very high. Fair enough a working individual is mostly a right fit for any kind of loan.

## Checking the number of successful applicant per Income Type

sns.countplot(df,x='Marital_status',hue='label',palette ='prism')
plt.xlabel('Marital Status')
plt.ylabel('Count')
plt.title('Approval Numbers per Marital status')
plt.xticks(rotation=45)
plt.show()

## We can see that overall the approval rate and the number of applicants are high for Married individuals.

## Checking the ratio of Male and Females applicants and approval ratio

sns.countplot(df,x='GENDER',hue='label',palette ='magma')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Approval Numbers per Gender')
plt.show()

## We can clearly see that the number of Female applicatants are way more than that of Male, also the rate of Approval for Female is higher.

## Checking the ratio of Car Owners applicants and approval ratio

sns.countplot(df,x='Car_Owner',hue='label',palette ='turbo')
plt.xlabel('Car Owner')
plt.ylabel('Count')
plt.title('Approval Numbers per Car Owners')
plt.show()

## We can see that the number of Non car owners applicant being approved is high and also the non-approval rate is little bit high comparatively, as it should be because of high numbers of applicants.

## Checking the ratio of Car Owners applicants and approval ratio

sns.countplot(df,x='Propert_Owner',hue='label',palette ='gist_stern')
plt.xlabel('Property Owner')
plt.ylabel('Count')
plt.title('Approval Numbers per Property Owners')
plt.show()

## We can see that the number of Property owners applicant is high and also the approval rate is little high, as it should be because of high numbers of applicants.

## Checking the number of successful applicant per Education

sns.countplot(df,x='EDUCATION',hue='label')
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.title('Approval Numbers per Education')
plt.xticks(rotation=45)
plt.show()

## We can see here clearly that people with atleast Secondary edductaion has the most chances of getting approval. Wheras, people who has not completed the secondary degree has less chance of approval.

### These all graphs are just to show patterns on indicvidual level of features,although we will need a mix of all these features to predict the actual outcomes, ie, approval.

### Now we will make certain changes to the dataset, will drop some not required column and encode a few

### Dropping not needed column

df.head()

df.columns

## We have already made better column from Birthday Count and Employed days, so we will not need these further
## Will drop the column

df.drop(columns=['Birthday_count','Employed_days'],axis=1,inplace=True)

# rechecking columns

df.columns

### Downloadng the clean dataframe for SQL analysis

df.to_csv('new_credit_card_full_data.csv',index=False)

## We will use Feature selection here, but before that we have some of our initial hypothesis to do.

"""### Assumption and Hypothesis - (Section 2 - Question 2)"""

### Initial Hypothesis

#1) The hypothesis is that Annual Income and Credit Approval are not highly correlated, and we will do hypothsis test to get the result.
# The null hypothesis (H0) is that there is no significant correlation.
# While the alternative hypothesis (H1) is that there is a significant correlation.
# Test Method:'T-Test' -  Will compare the mean Annual Income for approved and rejected credit applications.

## importing library

from scipy.stats import ttest_ind

# Seperating annual income for the approved and non-approved ones

approved_income = df[df['label']==0]['Annual_income']
rejected_income = df[df['label']==1]['Annual_income']

rejected_income

approved_income

## applying the T-test for the created values

t_stat,p_value1 = ttest_ind(approved_income,rejected_income )    # we will be comparing the means, in order to get the final score

## Lets check the result
print('Statistics: ',t_stat)
print('P value: ',p_value1)

## assigning the significance level to compare

sl = 0.05

# Comparing value

if p_value1 < sl:
  print("Reject the null hypothesis. There is a significant difference in mean Annual Income.")
else:
  print("Fail to reject the null hypothesis. There is no significant difference in mean Annual Income.")

# There is No significant in the Mean Annual Income for both the groups

##2) The hypothesis is that Marital Status and Credit Approval are not highly correlated, and we will do hypothsis test to get the result.
# Null hypothesis - H0 - There is no association between Marital Status and Credit Approval.
# While the alternative hypothesis (H1) there is an association between Marital Status and Credit Approval..
# Test Method:'Chi-Squared test' -  Will wompare two categorical values here.

## importing library

from scipy.stats import chi2_contingency

# Making use of a crosstab to create a contingency table, to shows the counts of individuals in different categories

contingency_table = pd.crosstab(df['Marital_status'], df['label'])

contingency_table

## Doing the Chi-Squared test

chi2_stat, p_value2, dof, expected = chi2_contingency(contingency_table)

## Getting the results

print(f"Chi-square statistic: {chi2_stat}")
print(f"P-value: {p_value2}")
print(f"Degrees of freedom: {dof}")

## Comparing value

sl = 0.05
if p_value2 < sl:
    print("Reject the null hypothesis. There is a significant association.")
else:
    print("Fail to reject the null hypothesis. There is no significant association.")

## We can clearly see that is a proper relation between Marital status and Approval rate. So this is a very important feature to keep in the model.

##3) The hypothesis is that Education level and Credit Approval are not highly correlated, and we will do hypothsis test to get the result.
# Null hypothesis - H0 - There is no association between Education and Credit Approval.
# While the alternative hypothesis (H1) there is an association between Education and Credit Approval..
# Test Method:'Chi-Squared test' -  Will wompare two categorical values here.

# Making use of a crosstab to create a contingency table, to shows the counts of individuals in different categories

contingency_table2 = pd.crosstab(df['EDUCATION'], df['label'])

contingency_table2

## Doing the Chi-Squared test

chi2_stat2, p_value3, dof2, expected = chi2_contingency(contingency_table2)

## Getting the results

print(f"Chi-square statistic: {chi2_stat2}")
print(f"P-value: {p_value3}")
print(f"Degrees of freedom: {dof2}")

## Comparing value

sl = 0.05
if p_value3 < sl:
    print("Reject the null hypothesis. There is a significant association.")
else:
    print("Fail to reject the null hypothesis. There is no significant association.")

## We can see that is No proper relation between Education and Approval rate.

### Data Preparation

df.head()

df.columns

df['EDUCATION'].unique()

## Cheking value counts
df['EDUCATION'].value_counts()

# We are deciding to take this column as a Ordinal column with the highest level of education being Academic degree and lowest being the Lower secondary.

## Lets do the Mapping of the Eduaction column. Ordinal Encoding

# Creating a Dictionary

education_mapping = {
    'Lower secondary': 1,
    'Secondary / secondary special': 2,
    'Incomplete higher': 3,
    'Higher education': 4,
    'Academic degree': 5
}

## Mapping the value on a new Education column

df['Education_Encoded'] = df['EDUCATION'].map(education_mapping)

df['Education_Encoded'].isnull().sum()

df['Education_Encoded'].value_counts()

# Checking the dataset

df.head()

## Lets do Label Encoding

## We will encode the categorical values, the nominal values.

## Using pd.getdummies to encoded the categorical values

df_encoded = pd.get_dummies(df, columns=['GENDER', 'Car_Owner', 'Propert_Owner', 'Type_Income', 'Marital_status', 'Housing_type', 'Type_Occupation'], drop_first=True)

# checking new dataset

df_encoded.head()

df_encoded.info()

df_encoded.columns

## Checking nulls

df_encoded.isnull().sum()

## No null values

## We will not need Age group and EDUCATION column now, so will drop it

df_encoded.drop(columns=['Age_Group','EDUCATION'],axis=1,inplace=True)

df_encoded.columns

df_encoded.head()

### Now we will do Feature Selection

# Here we'll use Recursive Feature Elimination (RFE) with a Random Forest classifier model. RFE does well with both categorical and numeric fearures and we will be using cross validation technique to get the best features at the end of the process.
# This will keep the best fearures and will remove the least important ones.

# We are doing feature selection first so as to decrease the dimensionality of the dataset while keeping the essence of it, this will help the machine to run efficiently while doing further process.

## importing libraries
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestClassifier   # for model creation
from sklearn.model_selection import train_test_split  # to split the data
from sklearn import metrics                           # to get the score of the models made with difference features

# Seperating datasets for training and testing
X = df_encoded.drop('label', axis=1)
y = df_encoded['label']

## Splitting for test and traing

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# creating model object

rffc = RandomForestClassifier(random_state=1)

# Use RFE with cross validation for feature selection

rfecv = RFECV(estimator=rffc, step=1, cv=5)  # we generally take cross fold as 5 and it will have different subsets of fearures to evaluate upon
X_train_rfecv = rfecv.fit_transform(X_train, y_train)    # step means the number of features the model will remove on each iteration

# to include only the selected features that we got from above steps, so that same features are used in both train and test

X_test_rfecv = rfecv.transform(X_test)

## Finally training the model on the selected features

rffc.fit(X_train_rfecv, y_train)

## Now we will get the prediction

y_pred0 = rffc.predict(X_test_rfecv)

## Checking the result

accuarcy_with_rfecv = metrics.accuracy_score(y_test,y_pred0)
accuarcy_with_rfecv

# We got a good accuracy with the selected features.

# We will get selected features

selected_features_rfecv = X.columns[rfecv.support_]

## Lets find the selected features which are best for our model and resources

print("Selected Features (RFECV): ",selected_features_rfecv)

### So, we have finally got the most important features and we will use the from now onwards to actually train and test our models

"""## MODEL TRAINING AND TESTING"""

## We will keep the selected and import features on to make all the necessary models
## So, we will split the data for train and test and split
## We will need to Scale the data because some of our models will need a scaled data to work upon, like logistic regression model

## Lets take on the important feature for the split

X_selected = df_encoded[selected_features_rfecv]
y = df_encoded['label']

# Dropping the Ind_id column

X_selected = X_selected.drop(columns=['Ind_ID','Type_Occupation_Not Specified'],axis=1)

## Doing train and test split

X_train,X_test,y_train,y_test = train_test_split(X_selected,y,test_size=0.25,random_state=0)

X_train.head()

y_train

## We will have to Scale the datasets

# Will use StandardScaler to do the same, since we have few outliers in columns like Annual income, Standardisation can be a little more safe here.

# importing libraries
from sklearn.preprocessing import StandardScaler

# making an object
ss = StandardScaler()

# scaling the training sets
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.fit_transform(X_test)

"""### Logistic Regression Model"""

## importing libraries

from sklearn.linear_model import LogisticRegression

# making an object

lr = LogisticRegression(random_state=42)

## Training the model

lr.fit(X_train_scaled,y_train)

## Lets check whether target variable is balanced or not to decide on the metrics to judge the result

df_encoded['label'].value_counts()

# This is a imbalance set , so we won't be using Accuracy alone here, instead, we will be using all the precision, recall, f1 and confusion matrix to get the best output result standard

## Getting the prediction

lr_y_pred = lr.predict(X_test_scaled)

## Comparing the results

lr_accuracy = metrics.accuracy_score(y_test,lr_y_pred)
lr_precision = metrics.precision_score(y_test, lr_y_pred)
lr_recall = metrics.recall_score(y_test, lr_y_pred)
lr_f1 = metrics.f1_score(y_test, lr_y_pred)
lr_auc_roc = metrics.roc_auc_score(y_test,lr_y_pred)

print('Accuracy',metrics.accuracy_score(y_test,lr_y_pred))
print('Precision',metrics.precision_score(y_test, lr_y_pred))
print('Recall',metrics.recall_score(y_test, lr_y_pred))
print('F1',metrics.f1_score(y_test, lr_y_pred))
print('AUC-ROC',lr_auc_roc)
print(metrics.classification_report(y_test, lr_y_pred))
print(metrics.confusion_matrix(y_test, lr_y_pred))

# Interpretation

#The accuracy here is good and even precision is good, but the recall and hence the f1 score came out to be very poor. This is mainly because of the class imbalance in the target variable.

"""### Support Vector Classifier"""

## importing libraries

from sklearn.svm import SVC

# making an object

svc = SVC(random_state=42)

## Training the model

svc.fit(X_train_scaled,y_train)

## Getting the prediction

svc_y_pred = svc.predict(X_test_scaled)

## Comparing the results

svc_accuracy = metrics.accuracy_score(y_test,svc_y_pred)
svc_precision = metrics.precision_score(y_test, svc_y_pred)
svc_recall = metrics.recall_score(y_test, svc_y_pred)
svc_f1 = metrics.f1_score(y_test, svc_y_pred)
svc_auc_roc = metrics.roc_auc_score(y_test,svc_y_pred)

print('Accuracy',svc_accuracy)
print('Precision',svc_precision)
print('Recall',svc_recall)
print('F1',svc_f1)
print('AUC-ROC',svc_auc_roc)
print(metrics.classification_report(y_test, svc_y_pred))
print(metrics.confusion_matrix(y_test, svc_y_pred))

# Interpretation

#The accuracy here is good and even precision is good, but the recall and hence the f1 is 0. Which is not that good. This is mainly because of the class imbalance in the target variable.

"""### Random Forest Classifier"""

## importing libraries

from sklearn.ensemble import RandomForestClassifier

# making an object

rfc = RandomForestClassifier(random_state=42)

## Training the model

rfc.fit(X_train_scaled,y_train)

## Getting the prediction

rfc_y_pred = rfc.predict(X_test_scaled)

## Comparing the results

rfc_accuracy = metrics.accuracy_score(y_test,rfc_y_pred)
rfc_precision = metrics.precision_score(y_test, rfc_y_pred)
rfc_recall = metrics.recall_score(y_test, rfc_y_pred)
rfc_f1 = metrics.f1_score(y_test, rfc_y_pred)
rfc_auc_roc = metrics.roc_auc_score(y_test,rfc_y_pred)

print('Accuracy',rfc_accuracy)
print('Precision',rfc_precision)
print('Recall',rfc_recall)
print('F1',rfc_f1)
print('AUC-ROC ',rfc_auc_roc)
print(metrics.classification_report(y_test, rfc_y_pred))
print(metrics.confusion_matrix(y_test, rfc_y_pred))

# Interpretation

#The accuracy here is better here and even precision is good, but the recall and hence the f1 score still came out to be very poor. This is mainly because of the class imbalance in the target variable.

"""### Decision Tree"""

## importing libraries

from sklearn.tree import DecisionTreeClassifier

# making an object

dtc = DecisionTreeClassifier(random_state=42)

## Training the model

dtc.fit(X_train_scaled,y_train)

## Getting the prediction

dtc_y_pred = dtc.predict(X_test_scaled)

## Comparing the results

dtc_accuracy = metrics.accuracy_score(y_test,dtc_y_pred)
dtc_precision = metrics.precision_score(y_test, dtc_y_pred)
dtc_recall = metrics.recall_score(y_test, dtc_y_pred)
dtc_f1 = metrics.f1_score(y_test, dtc_y_pred)
dtc_auc_roc = metrics.roc_auc_score(y_test,dtc_y_pred)

print('Accuracy',dtc_accuracy)
print('Precision',dtc_precision)
print('Recall',dtc_recall)
print('F1',dtc_f1)
print('AUC-ROC ',dtc_auc_roc)
print(metrics.classification_report(y_test, dtc_y_pred))
print(metrics.confusion_matrix(y_test, dtc_y_pred))

# Interpretation

#The accuracy here is okay here , but the precision,recall and hence the f1 score came out to be very poor. This is mainly because of the class imbalance in the target variable.

"""### Gradient Boosting"""

## importing libraries

from sklearn.ensemble import GradientBoostingClassifier

# making an object

gbc = GradientBoostingClassifier(random_state=42)

## Training the model

gbc.fit(X_train_scaled,y_train)

## Getting the prediction

gbc_y_pred = gbc.predict(X_test_scaled)

## Comapring the results

gbc_accuracy = metrics.accuracy_score(y_test,gbc_y_pred)
gbc_precision = metrics.precision_score(y_test, gbc_y_pred)
gbc_recall = metrics.recall_score(y_test, gbc_y_pred)
gbc_f1 = metrics.f1_score(y_test, gbc_y_pred)
gbc_auc_roc = metrics.roc_auc_score(y_test,gbc_y_pred)

print('Accuracy',gbc_accuracy)
print('Precision',gbc_precision)
print('Recall',gbc_recall)
print('F1',gbc_f1)
print('AUC-ROC ',gbc_auc_roc)
print(metrics.classification_report(y_test, gbc_y_pred))
print(metrics.confusion_matrix(y_test, gbc_y_pred))

# Interpretation

#The accuracy here is good and even precision is good, but the recall and hence the f1 score is near 0 only, so very poor. This is mainly because of the class imbalance in the target variable.

"""### XGBoost"""

## importing libraries

from xgboost import XGBClassifier

# making an object

xgb = XGBClassifier(random_state=42)

## Training the model

xgb.fit(X_train_scaled,y_train)

## Getting the prediction

xgb_y_pred = xgb.predict(X_test_scaled)

## Comapring the results

xgb_accuracy = metrics.accuracy_score(y_test,xgb_y_pred)
xgb_precision = metrics.precision_score(y_test, xgb_y_pred)
xgb_recall = metrics.recall_score(y_test, xgb_y_pred)
xgb_f1 = metrics.f1_score(y_test, xgb_y_pred)  # the harmonic mean of recall and precision
xgb_auc_roc = metrics.roc_auc_score(y_test, xgb_y_pred)  # shows the ability of the model to differentiate between different classes

print('Accuracy',xgb_accuracy)
print('Precision',xgb_precision)
print('Recall',xgb_recall)
print('F1',xgb_f1)
print(metrics.classification_report(y_test, xgb_y_pred))
print(metrics.confusion_matrix(y_test, xgb_y_pred))
print(metrics.roc_auc_score(y_test, xgb_y_pred))

# Interpretation

#The accuracy here is good, precision, recall and hence the f1 score came out to be very poor. This is mainly because of the class imbalance in the target variable.

"""### K-Nearest Neighbor"""

## importing libraries

from sklearn.neighbors import KNeighborsClassifier

# making an object

knc = KNeighborsClassifier()

## Training the model

knc.fit(X_train_scaled,y_train)

## Getting the prediction

knc_y_pred = knc.predict(X_test_scaled)

## Comapring the results

knc_accuracy = metrics.accuracy_score(y_test,knc_y_pred)
knc_precision = metrics.precision_score(y_test, knc_y_pred)
knc_recall = metrics.recall_score(y_test, knc_y_pred)
knc_f1 = metrics.f1_score(y_test, knc_y_pred)
knc_auc_roc = metrics.roc_auc_score(y_test, knc_y_pred)

print('Accuracy',knc_accuracy)
print('Precision',knc_precision)
print('Recall',knc_recall)
print('F1',knc_f1)
print(metrics.classification_report(y_test, knc_y_pred))
print(metrics.confusion_matrix(y_test, knc_y_pred))

# Interpretation

# The accuracy here is good, precision, recall and hence the f1 score is near 0 only, so very poor. This is mainly because of the class imbalance in the target variable.

"""Models: RandomForest , SVC and KNN performed well overall.

### We will do Hyper parameter tuning with GridSearch Cv in order to improve the models, that performed fairly well out of all the above
"""

#### Will With work with LogisticReression, DecisionTree, RandomForest, XGBoost,KNN

"""### Logistic Regression with GridSearchCV



"""

## importing library

from sklearn.model_selection import GridSearchCV

# Defining parameters

param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

# C is how much regularized we want the model to be

# Making new object

lr_gd = LogisticRegression(max_iter=1000)

# Creating with GridSearchCV with accuracy as scoring

grid_search_lr = GridSearchCV(lr_gd,param_grid_lr,scoring='accuracy',cv=5)

## Training the model

grid_search_lr.fit(X_train_scaled,y_train)

# Geting Best Parameters
best_params_lr = grid_search_lr.best_params_

best_params_lr

#Getting the prediction
y_pred_lr_gd = grid_search_lr.predict(X_test_scaled)

## Comapring the results

grid_lr_accuracy = metrics.accuracy_score(y_test,y_pred_lr_gd)
grid_lr_precision = metrics.precision_score(y_test, y_pred_lr_gd)
grid_lr_recall = metrics.recall_score(y_test, y_pred_lr_gd)
grid_lr_f1 = metrics.f1_score(y_test, y_pred_lr_gd)
grid_lr_roc = metrics.roc_auc_score(y_test, y_pred_lr_gd)

print('Accuracy',grid_lr_accuracy)
print('Precision',grid_lr_precision)
print('Recall',grid_lr_recall)
print('F1',grid_lr_f1)
print('AUC-ROC ', metrics.roc_auc_score(y_test, y_pred_lr_gd))
print(metrics.classification_report(y_test, y_pred_lr_gd))
print(metrics.confusion_matrix(y_test, y_pred_lr_gd))

## The accuracy is still good, but other metrics are just not working.

"""### Decision Tree with GridSearchCV"""

## Defining parameters

param_grid_dtc = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# In criterion, we gave both Genie Impurity and Entrophy to select best nodes
# max_depth, is maximum depth of the tree
# min_sample_split, minimum number of sample required to split the node
# min_sample_leaf , minimum number of sample required to be a leaf node

## Creating a new object

dtc_gd = DecisionTreeClassifier()

# Making GridSearchCV with Accuracy scoring

grid_search_dtc = GridSearchCV(dtc_gd, param_grid_dtc, scoring='accuracy', cv=5)

#Training the dataset
grid_search_dtc.fit(X_train_scaled, y_train)

## Getting best parameters

best_params_dtc = grid_search_dtc.best_params_

best_params_dtc

## Getting the predictions

y_pred_dtc_gd = grid_search_dtc.predict(X_test_scaled)

## Comparing the results

grid_dtc_accuracy = metrics.accuracy_score(y_test,y_pred_dtc_gd)
grid_dtc_precision = metrics.precision_score(y_test, y_pred_dtc_gd)
grid_dtc_recall = metrics.recall_score(y_test, y_pred_dtc_gd)
grid_dtc_f1 = metrics.f1_score(y_test, y_pred_dtc_gd)
grid_dtc_roc = metrics.roc_auc_score(y_test, y_pred_dtc_gd)

print('Accuracy',grid_dtc_accuracy)
print('Precision',grid_dtc_precision)
print('Recall',grid_dtc_recall)
print('F1',grid_dtc_f1)
print('AUC-ROC ',grid_dtc_roc)
print(metrics.classification_report(y_test, y_pred_dtc_gd))
print(metrics.confusion_matrix(y_test, y_pred_dtc_gd))

# As we can see that the Accuracy, recall and precision for the '0' class is around 93%, which means that the model is actually performing very good in predicting the class '0', but when it comes to the prediction of '1', the model is only predicting properly around 45% of times. This has to be better.
# Note: DecisionTree models are prone to overfitting.

"""### Random Forest with GridSearchCV"""

## Defining parameters

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# n_estimator, number of trees in the randomforest
# max_depth, the depth of the trea nodes

## Creating a new object

rf_gd = RandomForestClassifier()

# Making GridSearchCV with Accuracy scoring

grid_search_rf = GridSearchCV(rf_gd, param_grid_rf, scoring='accuracy', cv=5)

#Training the dataset
grid_search_rf.fit(X_train_scaled, y_train)

## Getting best parameters

best_params_rf = grid_search_rf.best_params_

best_params_rf

## Getting the predictions

y_pred_rf_gd = grid_search_rf.predict(X_test_scaled)

## Comapring the results

grid_rf_accuracy = metrics.accuracy_score(y_test,y_pred_rf_gd)
grid_rf_precision = metrics.precision_score(y_test, y_pred_rf_gd)
grid_rf_recall = metrics.recall_score(y_test, y_pred_rf_gd)
grid_rf_f1 = metrics.f1_score(y_test, y_pred_rf_gd)
grid_rf_roc = metrics.roc_auc_score(y_test, y_pred_rf_gd)


print('Accuracy',grid_rf_accuracy)
print('Precision',grid_rf_precision)
print('Recall',grid_rf_recall)
print('F1',grid_rf_f1)
print('AUC-ROC ',grid_rf_roc)
print(metrics.classification_report(y_test, y_pred_rf_gd))
print(metrics.confusion_matrix(y_test, y_pred_rf_gd))

# As we can see that the Accuracy, recall and precision is around 90% This can be made better.

"""### XGBClassifier with GridSearchCV"""

## Defining parameters

param_grid_xgb = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}



## Creating a new object

xgb_gd = XGBClassifier()

# Making GridSearchCV with Accuracy scoring

grid_search_xgb = GridSearchCV(xgb_gd, param_grid_xgb, scoring='accuracy', cv=5)

#Training the dataset
grid_search_xgb.fit(X_train_scaled, y_train)

## Getting best parameters

best_params_xgb = grid_search_xgb.best_params_

best_params_xgb

## Getting the predictions

y_pred_xgb_gd = grid_search_xgb.predict(X_test_scaled)

## Comapring the results

grid_xgb_accuracy = metrics.accuracy_score(y_test,y_pred_xgb_gd)
grid_xgb_precision = metrics.precision_score(y_test, y_pred_xgb_gd)
grid_xgb_recall = metrics.recall_score(y_test, y_pred_xgb_gd)
grid_xgb_f1 = metrics.f1_score(y_test, y_pred_xgb_gd)
grid_xgb_roc = metrics.roc_auc_score(y_test, y_pred_xgb_gd)



print('Accuracy',grid_xgb_accuracy)
print('Precision',grid_xgb_precision)
print('Recall',grid_xgb_recall)
print('F1',grid_xgb_f1)
print('AUC-ROC ',grid_xgb_roc)
print(metrics.classification_report(y_test, y_pred_xgb_gd))
print(metrics.confusion_matrix(y_test, y_pred_xgb_gd))

# In this model the accuracy,recall and precision is round 90%. But the f1 score and AUC-ROC is both low.

"""### Support Vector Classifier with GridSearchCV"""

## Defining parameters

param_grid_svc = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}



## Making a new object

svc_gd = SVC()

# Making GridSearchCV with Accuracy scoring

grid_search_svc = GridSearchCV(svc_gd, param_grid_svc, scoring='accuracy', cv=5)

#Training the dataset
grid_search_svc.fit(X_train_scaled, y_train)

## Getting best parameters

best_params_svc = grid_search_svc.best_params_

best_params_svc

## Getting the predictions

y_pred_svc_gd = grid_search_svc.predict(X_test_scaled)

## Comapring the results

grid_svc_accuracy = metrics.accuracy_score(y_test,y_pred_svc_gd)
grid_svc_precision = metrics.precision_score(y_test, y_pred_svc_gd)
grid_svc_recall = metrics.recall_score(y_test, y_pred_svc_gd)
grid_svc_f1 = metrics.f1_score(y_test, y_pred_svc_gd)
grid_svc_roc = metrics.roc_auc_score(y_test, y_pred_svc_gd)



print('Accuracy',grid_svc_accuracy)
print('Precision',grid_svc_precision)
print('Recall',grid_svc_recall)
print('F1',grid_svc_f1)
print('AUC-ROC ',grid_svc_roc)
print(metrics.classification_report(y_test, y_pred_svc_gd))
print(metrics.confusion_matrix(y_test, y_pred_svc_gd))

# Overall accuracy of the model on the entire dataset is 90%.Accuarcy,Precision and recall is also oaky.F1 score and ROC is better than other ones.

"""### K Neighbor with GridSearchCV"""

## Defining parameters

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

## Making a new object

knn_gd = KNeighborsClassifier()

# Making GridSearchCV with Accuracy scoring

grid_search_knn = GridSearchCV(knn_gd, param_grid_knn, scoring='accuracy', cv=5)

#Training the dataset
grid_search_knn.fit(X_train_scaled, y_train)

## Getting best parameters

best_params_knn = grid_search_knn.best_params_

best_params_knn

## Getting the predictions

y_pred_knn_gd = grid_search_knn.predict(X_test_scaled)

## Comapring the results

grid_knn_accuracy = metrics.accuracy_score(y_test,y_pred_knn_gd)
grid_knn_precision = metrics.precision_score(y_test, y_pred_knn_gd)
grid_knn_recall = metrics.recall_score(y_test, y_pred_knn_gd)
grid_knn_f1 = metrics.f1_score(y_test, y_pred_knn_gd)
grid_knn_roc = metrics.roc_auc_score(y_test, y_pred_knn_gd)



print('Accuracy',grid_knn_accuracy)
print('Precision',grid_knn_precision)
print('Recall',grid_knn_recall)
print('F1',grid_knn_f1)
print('AUC-ROC ',grid_knn_roc)
print(metrics.classification_report(y_test, y_pred_knn_gd))
print(metrics.confusion_matrix(y_test, y_pred_knn_gd))

## Model performed like SVC only.

### Making a DataFrame of all the results

## Initial Model Results

initial_result = {'Models':['Logistic Regress','SVC','Decision Tree','Random Forest','XGBoost','K Neighbor'],
                  'Accuracy':[lr_accuracy,svc_accuracy,dtc_accuracy,rfc_accuracy,xgb_accuracy,knc_accuracy],
                  'Precision':[lr_precision,svc_precision,dtc_precision,rfc_precision,xgb_precision,knc_precision],
                  'Recall':[lr_recall,svc_recall,dtc_recall,rfc_recall,xgb_recall,knc_accuracy],
                  'F1 Score':[lr_f1,svc_f1,dtc_f1,rfc_f1,xgb_f1,knc_f1],
                  'ROC-AUC':[lr_auc_roc,svc_auc_roc,dtc_auc_roc,rfc_auc_roc,xgb_auc_roc,knc_auc_roc]
                  }

initial_result_df = pd.DataFrame(initial_result)

initial_result_df

## Mid Model Results

Mid_result =  {'Models':['Logistic Regress','SVC','Decision Tree','Random Forest','XGBoost','K Neighbor'],
                  'Accuracy':[grid_lr_accuracy,grid_svc_accuracy,grid_dtc_accuracy,grid_rf_accuracy,grid_xgb_accuracy,grid_knn_accuracy],
                  'Precision':[grid_lr_precision,grid_svc_precision,grid_dtc_precision,grid_rf_precision,grid_xgb_precision,grid_knn_precision],
                  'Recall':[grid_lr_recall,grid_svc_recall,grid_dtc_recall,grid_rf_recall,grid_xgb_recall,grid_knn_accuracy],
                  'F1 Score':[grid_lr_f1,grid_svc_f1,grid_dtc_f1,grid_rf_f1,grid_xgb_f1,grid_knn_f1],
                  'ROC-AUC':[grid_lr_roc,grid_svc_roc,grid_dtc_roc,grid_rf_roc,grid_xgb_roc,grid_knn_roc]
                  }

Mid_result_df = pd.DataFrame(Mid_result)

Mid_result_df

"""Section 4 - Question 4

Model comparison
"""

### Conclusion - Initial

"""Let's consider multiple metrics, including Accuracy, Precision, Recall, F1 Score, and ROC-AUC, to make a comprehensive evaluation of the models.

Considering multiple metrics:

- **Accuracy:** Both Random Forest and K Neighbors have the highest accuracy at approximately 90.96%, also SVC got around 90%.
- **Precision:** Random Forest has the highest precision at 80%, followed by K Neighbors at 68.75% and then SVC at around 55%.
- **Recall:** K Neighbors has the highest recall at 90.96%, indicating its ability to capture a high proportion of actual positive cases.
- **F1 Score:** K Neighbors has the highest F1 Score at 38.60%, which balances precision and recall, followed by the Random Forest and SVC model, at around 35%.
- **ROC-AUC:** Decision Tree has the highest ROC-AUC at 62.47%, followed by K Neighbors and SVC.

Now, if maximizing accuracy is the primary goal,Random Forest, K Neighbors might be preferred due to its higher accuracy, recall, and F1 Score and then followed by SVC.

However as we know the dataset is imblanaced and thus we need to make the data better and only after that we can consider the best Model via the Accuracy score.

So, we will further do Sampling and will make the data Balanced thus to get the Proper metrics out of the Accuracy score.

### We will perform Oversampling and then will perform Logistic,SVC,RFC and KNN as they performed well over all our test model
"""

## Importing libraries

# We will use oversampling method

from imblearn.over_sampling import RandomOverSampler

## Making an object

ros = RandomOverSampler()

## Doing sampling with our previously splitted data
# We will do oversampling only on training data, since the test data should match the real world values

X_train_resample, y_train_resample = ros.fit_resample(X_train_scaled, y_train)

## Checking the oversampled values
y_train_resample.value_counts()

# The values are balanced now

"""### SVC with Oversampled Data"""

### Keeping all the Hyper parameters same with Grid Search Cv.
## With train the data with Different datasets only

#Training the dataset
grid_search_svc.fit(X_train_resample, y_train_resample)

## Getting best parameters after resampling

best_params_svc_resampled = grid_search_svc.best_params_

best_params_svc_resampled

## Getting the predictions

y_pred_svc_gd_rs = grid_search_svc.predict(X_test_scaled)

## Comapring the results

grid_svc_accuracy_rs = metrics.accuracy_score(y_test,y_pred_svc_gd_rs)
grid_svc_precision_rs = metrics.precision_score(y_test, y_pred_svc_gd_rs)
grid_svc_recall_rs = metrics.recall_score(y_test, y_pred_svc_gd_rs)
grid_svc_f1_rs = metrics.f1_score(y_test, y_pred_svc_gd_rs)
grid_svc_roc_rs = metrics.roc_auc_score(y_test, y_pred_svc_gd_rs)



print('Accuracy',grid_svc_accuracy_rs)
print('Precision',grid_svc_precision_rs)
print('Recall',grid_svc_recall_rs)
print('F1',grid_svc_f1_rs)
print('AUC-ROC ',grid_svc_roc_rs)
print(metrics.classification_report(y_test, y_pred_svc_gd_rs))
print(metrics.confusion_matrix(y_test, y_pred_svc_gd_rs))

"""### RandomForest with OverSampled Data"""

#Training the dataset
grid_search_rf.fit(X_train_resample, y_train_resample)

## Getting best parameters

best_params_rf_resampled = grid_search_rf.best_params_

best_params_rf_resampled

## Getting the predictions

y_pred_rf_gd_rs = grid_search_rf.predict(X_test_scaled)

## Comapring the results

grid_rf_accuracy_rs = metrics.accuracy_score(y_test,y_pred_rf_gd_rs)
grid_rf_precision_rs = metrics.precision_score(y_test, y_pred_rf_gd_rs)
grid_rf_recall_rs = metrics.recall_score(y_test, y_pred_rf_gd_rs)
grid_rf_f1_rs = metrics.f1_score(y_test, y_pred_rf_gd_rs)
grid_rf_roc_rs = metrics.roc_auc_score(y_test, y_pred_rf_gd_rs)


print('Accuracy',grid_rf_accuracy_rs)
print('Precision',grid_rf_precision_rs)
print('Recall',grid_rf_recall_rs)
print('F1',grid_rf_f1_rs)
print('AUC-ROC ',grid_rf_roc_rs)
print(metrics.classification_report(y_test, y_pred_rf_gd_rs))
print(metrics.confusion_matrix(y_test, y_pred_rf_gd_rs))

"""### KNeighbor with ReSampled Data"""

#Training the dataset
grid_search_knn.fit(X_train_resample, y_train_resample)

## Getting best parameters

best_params_knn_resampled = grid_search_knn.best_params_

best_params_knn_resampled

## Getting the predictions

y_pred_knn_gd_rs = grid_search_knn.predict(X_test_scaled)

## Comparing the results

grid_knn_accuracy_rs = metrics.accuracy_score(y_test,y_pred_knn_gd_rs)
grid_knn_precision_rs = metrics.precision_score(y_test, y_pred_knn_gd_rs)
grid_knn_recall_rs = metrics.recall_score(y_test, y_pred_knn_gd_rs)
grid_knn_f1_rs = metrics.f1_score(y_test, y_pred_knn_gd_rs)
grid_knn_roc_rs = metrics.roc_auc_score(y_test, y_pred_knn_gd_rs)



print('Accuracy',grid_knn_accuracy_rs)
print('Precision',grid_knn_precision_rs)
print('Recall',grid_knn_recall_rs)
print('F1',grid_knn_f1_rs)
print('AUC-ROC ',grid_knn_roc_rs)
print(metrics.classification_report(y_test, y_pred_knn_gd_rs))
print(metrics.confusion_matrix(y_test, y_pred_knn_gd_rs))

### MODEL RESULTS

## Final Model Results

Final_results =  {'Models':['SVC','Random Forest','K Neighbor'],
                  'Accuracy':[grid_svc_accuracy_rs,grid_rf_accuracy_rs,grid_knn_accuracy_rs],
                  'Precision':[grid_svc_precision_rs,grid_rf_precision_rs,grid_knn_precision_rs],
                  'Recall':[grid_svc_recall_rs,grid_rf_recall_rs,grid_knn_accuracy_rs],
                  'F1 Score':[grid_svc_f1_rs,grid_rf_f1_rs,grid_knn_f1_rs],
                  'ROC-AUC':[grid_svc_roc_rs,grid_rf_roc_rs,grid_knn_roc_rs]
                  }

## Making it a DataFrame
Final_results_df = pd.DataFrame(Final_results)

## Results
Final_results_df

"""As discussed earlier,the overall goals of your model to Get the Best possible Accuracy in Predicting the Approval of Credit Applications

Let's check which model performed the best

## **FINAL CONCLUSION**

(Model comparison)

Note: Using OverSampling , we have removed the Imbalance in the Target variable for the training set and thus we can finally make the decision upon which Model works the best, on the basis of Accuracy Score.

Considering the provided metrics for Accuracy primarily and then will check with other metrics too.

Considering the metrics:

- **Accuracy:** Random Forest has the highest accuracy at 92.50%.
- **Precision:** Random Forest also has the highest precision at 87.50%.
- **Recall:** K Neighbors has the highest recall at 86.50%.
- **F1 Score:** All are having F1 Score at 49% - 50%.
- **ROC-AUC:** K Neighbors has the highest ROC-AUC at 76.37%.

The overall performance and the importance of different metrics, the **Random Forest model** comes to be a good choice. It achieves a good balance between accuracy, precision, and F1 Score, while maintaining a okay level of recall.

***Section 4 - Question 2:***

Among the considered models, the most appropriate choice for predicting credit card approval is the Random Forest Classifier.

Here's the justification:

**Ensemble Learning:**
Random Forest is an ensemble of decision trees, which helps improve predictive accuracy by reducing overfitting and handling noisy data.

**Robustness:**
Random Forest is robust to outliers and can handle a mix of numerical and categorical features without requiring extensive pre-processing.(Although we did it for the task)

**Reduction of Overfitting:**
By aggregating predictions from multiple decision trees, Random Forest minimises the risk of overfitting to the training data, leading to improved generalization on unseen data.

**Consistency and Stability:**
Random Forest tends to be less sensitive to variations in the dataset compared to individual decision trees, providing a stable and consistent performance.

**Hyperparameter Tuning:**
The use of GridSearchCV allows for fine-tuning the hyperparameters, optimizing the model's performance for the specific task of credit card approval prediction.


*Finally: The accuracy score is also the Highest in RandomForest Model.*

In summary, **Random Forest** is a well-rounded choice, balancing maximum accuracy, interpretability, and robustness. Its ability to handle various data complexities and provide insights into feature importance makes it suitable for this credit scoring task.

So, the **RandomForestClassifier** is the best model we have to accuartely predict the approval of the Credit.

Also, we have got the Best Params for the GridSearchCV, that can be used to create the final model that will work on the complete data.
"""



"""## **Feedback**
For further Model Improvements

The data can include few important features, so as to make it more robust and minimise the risk of miss prediction.

Things that can be included:
- Previous Credit History
- Derogatory Remarks
- Repayment Track Record
- Credit Equiries over a period of time
- Spending Behavior

These fearures will make the data more suitable for analysis.

Link to Document containing all the answer:


---

https://docs.google.com/document/d/1eV8kcByJbCWErvdMCMd2yKkZabFSMpJz-i14RlqWsTY/edit?usp=sharing
"""



"""### StreamLit App"""

pip install streamlit

import streamlit as st
import pickle

## We will save the trained model and that will be random forest classifier model, since it is the best model we have

# After fitting GridSearchCV
best_model = grid_search_rf.best_estimator_

# Save the trained model to a file

# Saving the best model to a file
with open('best_model_rf.pkl', 'wb') as f:
    pickle.dump(best_model, f)

# Loading the saved model from file
with open('best_model_rf.pkl', 'rb') as f:
    loaded_rf_model = pickle.load(f)

# Check the type of the loaded model
print(type(loaded_rf_model))

# Print the model
print(loaded_rf_model)

# Inspect attributes and methods of the model
print(dir(loaded_rf_model))

## Since the model hsa been loaded to do further prediction we will define a function to make prediction

# defining a function to make prediction
def predict_credit_approval(data):
  prediction = loaded_rf_model.predict(data)
  return prediction

## Creating streamlit app

# Creating input fields for user to enter data
st.title('Credit Card Approval Prediction')
gender = st.selectbox('Gender', ['Male', 'Female'])
car_owner = st.selectbox('Car Owner', ['Yes', 'No'])
annual_income = st.number_input('Annual Income', min_value=0)
education = st.selectbox('Education', ['Lower Secondary', 'Secondary / Secondary Special', 'Incomplete Higher', 'Higher Education', 'Academic Degree'])
age = st.number_input('Age', min_value=18, max_value=100)
property_owner = st.selectbox('Property Owner', ['Yes', 'No'])

# Converting input data to DataFrame
data = pd.DataFrame({
    'Gender': [gender],
    'Car_owner': [car_owner],
    'Annual_income': [annual_income],
    'Education': [education],
    'Age': [age],
    'Property_owner': [property_owner]
})

# This will make prediction when user clicks the 'Predict' button
if st.button('Predict'):
    prediction = loaded_rf_model.predict(data)
    if prediction[0] == 0:
        st.error('Application Rejected')
    else:
        st.success('Application Approved')

## We will do the same thing but now in a Main function so that Streamlit can deploy that

def main():
    st.title('Credit Card Approval Prediction')
    html_temp="""
    <div style="background-color:tomato;padding:10px">
    <h2 style="color:white;text-align:center;">Streamlit Credit Card Approval Predictor ML App </h2>

    """
    # to use html
    st.markdown(html_temp,unsafe_allow_html=True)

    # Load the trained model
    def load_model(model_file):
      with open(model_file, 'rb') as f:
        loaded_model = pickle.load(f)
      return loaded_model

    model_file = 'random_forest_model.pkl'
    rf_model = load_model(model_file)


    # Create input fields for user to enter data
    gender = st.selectbox('Gender', ['Male', 'Female'])
    car_owner = st.selectbox('Car Owner', ['Yes', 'No'])
    annual_income = st.number_input('Annual Income', min_value=0)
    education = st.selectbox('Education', ['Lower Secondary', 'Secondary / Secondary Special', 'Incomplete Higher', 'Higher Education', 'Academic Degree'])
    age = st.number_input('Age', min_value=18, max_value=100)
    property_owner = st.selectbox('Property Owner', ['Yes', 'No'])

    # Convert input data to DataFrame
    data = pd.DataFrame({
        'Gender': [gender],
        'Car_owner': [car_owner],
        'Annual_income': [annual_income],
        'Education': [education],
        'Age': [age],
        'Property_owner': [property_owner]
    })

    # Make prediction when user clicks the 'Predict' button
    if st.button('Predict'):
        prediction = rf_model.predict(data)
        if prediction[0] == 0:
            st.error('Application Rejected')
        else:
            st.success('Application Approved')

if __name__ == '__main__':
    main()

import streamlit as st
import pandas as pd
import pickle

def main():
    st.title('Credit Card Approval Prediction')
    html_temp="""
    <div style="background-color:tomato;padding:10px">
    <h2 style="color:white;text-align:center;">Streamlit Credit Card Approval Predictor ML App </h2>

    """
    # to use html
    st.markdown(html_temp,unsafe_allow_html=True)

    # Load the trained model
    def load_model(model_file):
        with open(model_file, 'rb') as f:
            loaded_model = pickle.load(f)
        return loaded_model

    # Specify the correct file path for the trained model
    model_file = 'best_model_rf.pkl'  # Update this with the correct file path
    rf_model = load_model(model_file)

    # Create input fields for user to enter data
    gender = st.selectbox('Gender', ['Male', 'Female'], key='gender_input')
    car_owner = st.selectbox('Car Owner', ['Yes', 'No'], key='car_owner_input')
    annual_income = st.number_input('Annual Income', min_value=0, key='income_input')
    work_phone = st.selectbox('Work Phone', ['Yes', 'No'], key='work_phone_input')
    phone = st.selectbox('Phone', ['Yes', 'No'], key='phone_input')
    family_members = st.number_input('Family Members', min_value=0, key='family_input')
    age = st.number_input('Age', min_value=18, max_value=100, key='age_input')
    experience = st.number_input('Experience', min_value=0, key='experience_input')
    education = st.selectbox('Education', ['Lower Secondary', 'Secondary / Secondary Special', 'Incomplete Higher', 'Higher Education', 'Academic Degree'], key='education_input')
    housing_type = st.selectbox('Housing Type', ['Municipal apartment', 'Other'], key='housing_input')
    occupation = st.selectbox('Occupation', ['Not Specified', 'Other'], key='occupation_input')
    type_income = st.selectbox('Type Income', ['Working', 'Other'], key='type_income_input')
    property_owner = st.selectbox('Property Owner', ['Yes', 'No'], key='property_owner_input')

    # Convert categorical input to numerical values
    gender_mapping = {'Male': 0, 'Female': 1}
    gender_encoded = gender_mapping[gender]

    education_mapping = {'Lower Secondary': 0, 'Secondary / Secondary Special': 1, 'Incomplete Higher': 2, 'Higher Education': 3, 'Academic Degree': 4}
    education_encoded = education_mapping[education]

    car_owner_encoded = 1 if car_owner == 'Yes' else 0
    work_phone_encoded = 1 if work_phone == 'Yes' else 0
    phone_encoded = 1 if phone == 'Yes' else 0
    housing_type_encoded = 1 if housing_type == 'Municipal apartment' else 0
    occupation_encoded = 1 if occupation == 'Not Specified' else 0
    type_income_encoded = 1 if type_income == 'Working' else 0
    property_owner_encoded = 1 if property_owner == 'Yes' else 0

    # Convert input data to DataFrame
    data = pd.DataFrame({
        'GENDER_M': [gender_encoded],
        'Car_Owner_Y': [car_owner_encoded],
        'Work_Phone': [work_phone_encoded],
        'Phone': [phone_encoded],
        'Family_Members': [family_members],
        'Age': [age],
        'Experience': [experience],
        'Education_Encoded': [education_encoded],
        'Type_Occupation_Not Specified': [occupation_encoded],
        'Type_Income_Working': [type_income_encoded],
        'Annual_income': [annual_income],
        'Housing_type_Municipal apartment': [housing_type_encoded],
        'Propert_Owner_Y': [property_owner_encoded]
    })

    # Make prediction when user clicks the 'Predict' button
    if st.button('Predict'):
        prediction = rf_model.predict(data)
        if prediction[0] == 0:
            st.error('Application Rejected')
        else:
            st.success('Application Approved')

if __name__ == '__main__':
    main()

